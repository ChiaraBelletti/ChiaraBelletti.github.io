---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Research interests
----- 
Applied Microeconomics, Digital Economics, Information Systems, Marketing, Platform Strategy, Quantitative Marketing.

Working Papers
----- 

* <a href="https://www.cesifo.org/en/publications/2024/working-paper/rating-systems-and-end-game-effect-when-reputation-works-and-when">"Rating Systems and the End-Game Effect: When Reputation Works and When it Doesn't"</a>, with Elizaveta Pronkina (Amazon) and Michelangelo Rossi (Télécom Paris), 2024 (Current Draft).
  * Reject & Resubmit, Marketing Science.
  * Draft: <a href="https://www.cesifo.org/en/publications/2024/working-paper/rating-systems-and-end-game-effect-when-reputation-works-and-when">CESifo Working Paper No. 11253, 2024.</a>
  * Abstract: Do rating systems provide incentives to sellers when they are about to exit a market? Using data from Airbnb, this paper examines how end-of-game considerations affect hosts effort decisions. We take advantage of a regulation on short-term rentals in the City of Los Angeles to identify hosts who
anticipated their imminent exit from the platform due to non-compliance with new eligibility rules.
We focus on hosts who left the platform as a result of the regulation and measure their effort with listings ratings in effort-related categories such as check-in, communication, and cleanliness. Using a Difference-in-Differences approach, we compare how effort-related ratings changed, relative to location ratings, after the regulation was announced and during its implementation.
Our analysis documents a statistically significant decline in effort during the final periods of hosts’ activity. By highlighting how end-of-game effects can
undermine the effectiveness of rating systems in incentivizing high-quality service provision, the paper offers valuable insights for platform managers designing reputation systems

Work in Progress
-----

* "How to Assess and Improve the Quality of Crowd-Sourced Data Work", with Louis Daniel Pape (Télécom Paris).
  * Draft: <a href="https://www.dropbox.com/scl/fi/i0vmamm0uu8bray6utjmz/Quality_MW.pdf?rlkey=01j6c33ylpprjsdtu8z9ye4wa&dl=0">available here.</a>
  * Abstract: Micro-tasking platforms enable the collection of data used to train machine learning algorithms and artificial intelligence. However, a classical Principal-Agent problem may limit the quality of the data produced by micro-taskers as firms do not always monitor the quality of the work done with sufficient frequency.
We develop a structural model of equilibrium demand and supply of effort to measure quality and monitoring behavior. We estimate the parameters of this model using proprietary data from a leading micro-tasking platform. We find that metrics relying on observed task rejection severely underestimate the quality/effort with which data annotation tasks are performed. This suggest AI is being built with mis-annotated data. We discuss several mitigation strategies. We find that increasing the pay of micro-taskers along with more frequent monitoring could help improve the quality of the data. Finally, we discuss incentive schemes to induce higher quality work by relying on counter-factual simulations. We show that charging penalties for workers with a rejected task could induce higher effort and require less monitoring from the firms.

* "Unveiling the Demand for Data Work in the Era of AI: Evidence from a Comprehensive Micro-Task Dataset", with Ulrich Laitenberger (Tilburg University) and Paola Tubaro (ENSAE, CREST).
  * Draft available upon request.
  * Abstract: This paper studies firm and worker behavior on a commercial crowd-working platform characterized by anonymity and limited employer-employee interactions and provides descriptive insights into how crowd-working platforms are used for outsourcing data-related work (e.g. AI training). The study begins by providing context on the platform under study and unveiling a recent growing demand for crowd-sourced data work. It also examines how firms ensure tasks’ execution quality through worker selection, wage setting, and monitoring. A regression framework allows for the identification of specific factors that distinguish demand for data work from other tasks. The higher targeting of demand towards predefined groups of contributors based on experience or geographic location, along with a larger rejection probability for data annotation tasks, underscores the importance of quality execution for firms outsourcing in this domain.

* "Can Platform Integration Mitigate Discovery Loss in the Digital Consumption Funnel?", with Denzel Glandel (LMU Munich) and Tobias Kretschmer (LMU Munich).
  * Draft available soon.

* "Replaceables? Profile Restarting on Digital Platforms. Evidence from the Short-Term Rental Market", with Joerg Claussen (LMU Munich) and Michail Batikas (NOVA Lisbon).
  * Draft available soon.

Research Projects
-----
* "Picky Drivers: Reputation and Selectivity on a Car-Pooling Platform", with Dianzhuo Zhu (University of Lille).


Policy Reports and Other Publications
-----
 
* "Crowdworking in France and Germany", with Ulrich Laitenberger, Daniel Erdsiek, and Paola Tubaro, 2021.
  * <a href="https://www.zew.de/publikationen/crowdworking-in-france-and-germany" target="_blank" rel="noopener noreferrer">ZEW expert brief Nr. 21-09. </a>

* Country chapter Italy, with Riccardo Norbiato (PSE) in Social Protection of Non-Standard Workers and the Self-Employed During the Pandemic, Spasova S., Ghailani D., Sabato S. and Vanhercke B.
   * <a href="https://www.etui.org/sites/default/files/2021-10/Social%20protection%20of%20non-standard%20workers%20and%20the%20self-employed%20during%20the%20pandemic-country%20chapters-2021.pdf" target="_blank" rel="noopener noreferrer">Available here.</a>
